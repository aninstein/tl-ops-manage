# tl openresty limit-fuse module

### 熔断限流分为两个模块，自检模块，配置同步模块。

## 1. 自检
   服务启动时，重载配置时自动刷新启动，。
   
   一份自检服务配置，会对该服务下的所有节点用同一份配置进行自检，自检配置提供如下字段

```
    {
        service_name = "service2",      #自检服务名称
        interval = 10 * 1000,           #自检周期， 默认单位/ms
        node_threshold = 0.3,           #节点阈值，如果某个节点路由（失败次数/路由总量）占比超过这个阈值，将对节点进行降级
        service_threshold = 0.5,        #服务阈值，如果某个服务下（节点熔断个数/节点总量）占比超过这个阈值，将对服务进行降级
        recover = 3 * 1000,             #单个自检周期内服务熔断后，自动恢复服务的周期。以此实现自动化熔断和限流
        depend = "token",               #限流依赖的策略模式，token:令牌桶限流，leak:漏桶限流（暂不支持）
        level = "service"               #自检层级，默认为服务(service)。包含服务下的所有节点
    }
```
#### 自动化熔断恢复实现思路
    
    在设计上，熔断应该拥有自我保护能力，做到无需人为干预，系统能根据当前请求情况进行自我调节流量状态。

    假设服务在某个时间点内流量激增，导致处理请求大批量超时或失败，短暂过后，服务自动重启恢复正常。
    
    或者在某个时间点，由于某些原因导致，某个节点挂掉，而已也没有触发自动重启。

    那么对于这些场景，我们在实现自动化熔断的时候，可以根据某个节点在某个周期内的负载量来衡量该服务的状态，并且该负载量仅在当前周期内有效，一个周期结束后，具体的服务节点状态应该被自动清除，从头开始继续统计服务负载状态。



    


## 2. 配置同步

服务支持动态新增，为保持动态扩展性，支持自检配置的动态同步。

包括新增服务自检配置，现有服务配置改动，两种情况下的配置平滑同步，无需reload nginx。
    
暂时支持增量配置，或配置变更的实时同步，后续逐步支持删除配置的实时同步


#### 配置同步实现思路

    依靠shared:dict共享内存实现，多个worker中依赖一个公共的配置版本号，若有某个worker检测到配置的新增或变动，自增版本号即可。
    
    其他worker自检前，与共享版本号对比自身配置版本号，若存在新增或变动，同步最新配置到worker内即可
    


