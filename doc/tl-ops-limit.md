# tl openresty limit-fuse module

### 熔断限流分为两个模块，自检模块，配置同步模块。

# 1. 自检
   服务启动时，重载配置时自动刷新启动，。
   
   一份自检服务配置，会对该服务下的所有节点用同一份配置进行自检，自检配置提供如下字段

```
    {
        service_name = "service2",      #自检服务名称
        interval = 10 * 1000,           #自检周期， 默认单位/ms
        node_threshold = 0.3,           #节点阈值，如果某个节点路由（失败次数/路由总量）占比超过这个阈值，将对节点进行降级
        service_threshold = 0.5,        #服务阈值，如果某个服务下（节点熔断个数/节点总量）占比超过这个阈值，将对服务进行降级
        recover = 3 * 1000,             #单个自检周期内服务熔断后，自动恢复服务的周期。以此实现自动化熔断和限流
        depend = "token",               #限流依赖的策略模式，token:令牌桶限流，leak:漏桶限流（暂不支持）
        level = "service"               #自检层级，默认为服务(service)。包含服务下的所有节点
    }
```
### 自动化熔断恢复实现思路
    
在设计上，熔断应该拥有自我保护能力，做到无需人为干预，系统能根据当前请求情况进行自我调节流量状态。

假设服务/节点在某个时间点内流量激增，导致处理请求大批量超时或失败，短暂过后，服务/节点自动重启恢复正常。
    
或者在某个时间点，由于某些原因导致，某个服务/节点挂掉，而已也没有触发自动重启。

    那么对于这些场景，我们在实现自动化熔断的时候，可以给服务定义出几个状态，正常状态，半熔断状态，全熔断状态，可以根据某个服务/节点在某个周期内的负载量来动态改变该服务/节点的状态
    
    并且该负载量仅在当前周期内有效，一个周期结束后，具体的服务/节点状态应该被自动清除，从头开始继续统计负载状态。

所以熔断状态可以用来统计某个服务/节点的性能指标


### 限流实现思路

限流的目的是为了保证某服务/节点在处理流量高峰期，或者性能欠佳的时候，对服务/节点的一种保护措施。

限流的应用场景应该基于当前服务/节点的性能状态而定，如果服务/节点性能指标正常，限流不必开启。
    
如果性能指标下降，应该对服务/节点减少流量进入，避免直接压垮服务。由此，限流可以在熔断状态的基础上进行流量控制

    在设计上，我们可以在服务状态处于某些状态下进行限流，限流的速率可以根据配置而定，如：达到半熔断状态，限流30%，达到全熔断状态，限流100%。

    对于限流实现，目前是支持令牌桶限流，后续将增加更多模式。

令牌桶配置如下

    {
        capacity = 10 * 1024 * 1024,      -- 最大容量 10M (按字节为单位，可做字节整型流控)
        rate = 1024,                      -- 令牌生成速率/秒 (每秒 1KB)
        warm = 100 * 1024,                -- 预热令牌数量 (预热100KB)
        half_capacity = 2                 -- 限流状态令牌最大容量
    }



# 2. 配置同步

服务支持动态新增，为保持动态扩展性，支持自检配置的动态同步。

包括新增服务自检配置，现有服务配置改动，两种情况下的配置平滑同步，无需reload nginx。
    
暂时支持增量配置，或配置变更的实时同步，后续逐步支持删除配置的实时同步


### 配置同步实现思路

    依靠shared:dict共享内存实现，多个worker中依赖一个公共的配置版本号，若有某个worker检测到配置的新增或变动，自增版本号即可。
    
    其他worker自检前，与共享版本号对比自身配置版本号，若存在新增或变动，同步最新配置到worker内即可
    


